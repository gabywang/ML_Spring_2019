{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "[Geron notebook](external/handson-ml/06_decision_trees.ipynb)\n",
    "\n",
    "[VanderPlass notebook](external/PythonDataScienceHandbook/notebooks/05.08-Random-Forests.ipynb)\n",
    "\n",
    "[scipy 2018](external/scipy-2018-sklearn/notebooks/18.In_Depth-Trees_and_Forests.ipynb)\n",
    "\n",
    "- Revisit Titanic Using decision trees\n",
    "    - code already done in [Non_Numerical_Data notebook](http://localhost:8888/notebooks/NYU/Non_Numerical_Data.ipynb#Random-Forest-Classifier)\n",
    "    \n",
    "[Geron equations](external/handson-ml/book_equations.ipynb#Chapter-6)\n",
    "\n",
    "[Geron ensemble and RandomForest](external/handson-ml/07_ensemble_learning_and_random_forests.ipynb)\n",
    "\n",
    "**NEED PICTURE**\n",
    "- Colored + and - in 2D\n",
    "- Show first split on one variable, and subsequent splits\n",
    "    - complicted decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to compute Purity of each variable that can be used to split\n",
    "- use pandas to create the 2 groups created by the split\n",
    "- count number in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plan\n",
    "- logic function to generate data\n",
    "- algo in English with abstract \"Choose variable\" step\n",
    "- describe recursive\n",
    "- use pandas slice/dice to isolate nodes\n",
    "    - get counts\n",
    "- use graphviz to generate slides\n",
    "- use Gini w/o justification\n",
    "    - Geron page 218 Gini\n",
    "    - Warning page 219: sklearn used CART, which is a binary tree (node has 2 children)\n",
    "        - other algos use ID3 which can have multiple children per node\n",
    "    - page 219: DecisionTrees highly interepretable; Forests not\n",
    "    - page 221: notation chose feature $k$ and threshold $t_k$ to do split\n",
    "        - greedy algo, so may not be best\n",
    "- optional features\n",
    "    - additional\tstopping\tconditions\t( min_samples_split ,\n",
    "min_samples_leaf ,\t min_weight_fraction_leaf ,\tand\t max_leaf_nodes\n",
    "- come back and use Info Gain\n",
    "    - entropy\n",
    "    - `DecisionTreeClassifier` choices: gini, entropy\n",
    "    - `DecisionTreeRegressor` choices: mse, mae\n",
    "        - variance reduction of split\n",
    "- Problems with trees\n",
    "- Bagging to overcome\n",
    "    - out of bag\n",
    "- Random forest\n",
    "    - bootstrap multiple datasets\n",
    "        - choose subset of variables\n",
    "            - weak learner\n",
    "- Boosting\n",
    "    - combine weak learners\n",
    "        - any model\n",
    "        - use stumps as example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree examples\n",
    "[compare classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py)\n",
    "- use Decision Tree on Moons and Linear Separable data\n",
    "- Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geron\n",
    "\n",
    "\n",
    "average number of bits in one sample  \n",
    "cross entropy   depends on message  encoding  \n",
    "average message  length\n",
    "goal  is to make messages  length  closer  to entropy  \n",
    "@5:00 variable  length  message  \n",
    "Huffman  coding  ?\n",
    "message  length is optimal for some other  distribution  \n",
    "so cross entopy gain measures  distance  between 2 distribution ? \n",
    "@7:00 cross entropy  like entropy  \n",
    "true prob times other distribution  \n",
    "so difference cross  entropy  is how m6ch closer we move target distribution  to true distribution \n",
    "KL divergence  \n",
    "cross entropy  equals entropy  when identical  distribution  \n",
    "one hot as true target distribution  \n",
    "so minimize  KL for objective  of classification  \n",
    "entropy  of one hot  is 0\n",
    "size of message  vs amount  of info\n",
    "negative  log is log of reciprocal  \n",
    "difference  log  in  log ratio  \n",
    "so gain is log ratio\n",
    "multinomial logitistc  classification  \n",
    "one hot target  vs one against all\n",
    "match distribution \n",
    "log softmax function  is cr8ss entropy ! \n",
    "since softmax is the probability  \n",
    "cross  entropy  better yhan accuracy  because  it weights with true probability  \n",
    "accuracy  is unweighted  \n",
    "information  gain\n",
    "Google  ml develop decision  trees \n",
    "@5:00\n",
    "gain  is reduction  in gini\n",
    "relation to shannon bits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "We will study Entropy for two reasons\n",
    "- an alternative to Gini for deciding which variable/what threshold to use in constructing a Decision Tree\n",
    "- a Cost measure for Classification (Cool Cost Function ?)\n",
    "\n",
    "- A measure of the randomness of a distribution\n",
    "- Interpreted as the number of bits of information obtained from a single sample\n",
    "\n",
    "## What is a bit ?\n",
    "- Amount of information need to reduce uncertainly by a factor of 2\n",
    "- Examples\n",
    "\n",
    "### Example: what is a bit\n",
    "\n",
    "Here, a bit is the amount of information **not** a length of the message\n",
    "\n",
    "### Average number of bits in a sample\n",
    "\n",
    "## Information gain\n",
    "- Reduction in entropy after having received some bits, compared to before receiving the bits\n",
    "\n",
    "## Cross entropy\n",
    "Measure of distance between two distributions.\n",
    "\n",
    "In Particular, One-Hot encoded target is a distribution with a single 1.\n",
    "\n",
    "Can use Cross Entropy as cost function in Multiomial Classification (don't have to use Logistic to classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Statquest: Josh Stamner \n",
    "- Entire sequence of videos excellent\n",
    "    - Decision Tree\n",
    "    - Random Forest parts 1, 2\n",
    "    - boosting (on trees, stumps)\n",
    "    \n",
    "- random forest\n",
    "    - bagging\n",
    "        - like cross validation\n",
    "        - bootstrap sample of observations (the \"bag\") from training set; unused observatios are called Out of Bag\n",
    "        - fit model on bag; evaluate on Out of Bag\n",
    "        - single prediction is average of predictions across the model fit on each bag\n",
    "            - and accuracy of an observation is evaluated\n",
    "                - only on models in which observation is out of bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube  Google  developers ML recipes  \n",
    "[link](https://www.youtube.com/watch?v=cKxRvEZd3Mw&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n",
    "\n",
    "[visualize data with Facets](https://pair-code.github.io/facets/?utm_campaign=ai_series_githubfacets_103017&utm_source=gdev&utm_medium=yt-desc%5C)\n",
    "\n",
    "decision  tree\n",
    "visualize   sklearn tools\n",
    "algo from scratch \n",
    "reduce  uncertainty  \n",
    "what makes a good  feature  \n",
    "stacked histogram \n",
    "\n",
    "recipe  4 tensorflow playground \n",
    "\n",
    "intro to feature  engineering  in tf\n",
    "age vs income  \n",
    "bucket as category  \n",
    "cross product features  , interactions \n",
    "easy for categorical \n",
    "\n",
    "weka\n",
    "accuracy  limitation  \n",
    "confusion  matrix  \n",
    "rank feature  by info gain like decision  tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMean score (10-fold validation: 0.789\n",
      "Model:  SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kjp/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMean score (10-fold validation: 0.732\n",
      "Model:  Decision Tree\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMean score (10-fold validation: 0.780\n",
      "Model:  Random Forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMean score (10-fold validation: 0.802\n"
     ]
    }
   ],
   "source": [
    "# %load code/titanic.py\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "# Sklearn imports\n",
    "\n",
    "TITANIC_PATH = os.path.join(\"./external/jack-dies\", \"data\")\n",
    "\n",
    "train_data = pd.read_csv( os.path.join(TITANIC_PATH, \"train.csv\") )\n",
    "test_data  = pd.read_csv( os.path.join(TITANIC_PATH, \"test.csv\")  )\n",
    "\n",
    "# Numeric pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# A class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "try:\n",
    "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
    "except ImportError:\n",
    "    from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "\n",
    "num_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector(num_features)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "\n",
    "# Categorical pipeline\n",
    "# Inspired from stackoverflow.com/questions/25239958\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)\n",
    "\n",
    "class SexToInt(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        sex = X[\"Sex\"]\n",
    "        X[\"Sex\"] = 0\n",
    "        X[ sex == \"female\" ] = 1\n",
    "        \n",
    "        return(X)\n",
    "\n",
    "cat_features =  [\"Pclass\", \"Sex\"]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(cat_features)),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"sex_encoder\", SexToInt() ),\n",
    "    ])\n",
    "\n",
    "# Union the numberic and categorical pipelines\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "# Run the pipelinem return an ndarray\n",
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "\n",
    "\n",
    "# Extract the training target\n",
    "target = \"Survived\"\n",
    "y_train = train_data[target]\n",
    "\n",
    "# Create models\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(gamma=\"auto\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "from sklearn import linear_model, preprocessing, model_selection \n",
    "\n",
    "logistic_clf = linear_model.LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "# Train models\n",
    "for name, clf in { \"Logistic\": logistic_clf,\n",
    "                   \"SVM\": svm_clf,\n",
    "                   \"Decision Tree\": tree_clf,\n",
    "                   \"Random Forest\": forest_clf\n",
    "                 }.items():\n",
    "    \n",
    "    print(\"Model: \", name)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = preprocess_pipeline.transform(test_data)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n",
    "    print(\"\\tMean score ({nf}-fold validation: {s:0.3f}\".format(\n",
    "            nf=num_folds,\n",
    "            s=scores.mean()\n",
    "            )\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'SibSp', 'Parch', 'Fare', 'Pclass', 'Sex']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['No', 'Yes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = \"titanic\"\n",
    "\n",
    "feature_names = num_features.copy()\n",
    "feature_names.extend(cat_features)\n",
    "\n",
    "target_names = [ \"No\", \"Yes\"]\n",
    "feature_names\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "PROJECT_ROOT_DIR=\"/tmp\"\n",
    "\n",
    "out_file=os.path.join(PROJECT_ROOT_DIR,\"{proj}_tree\".format(proj=proj)) \n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=out_file + \".dot\",\n",
    "        feature_names=feature_names,\n",
    "        class_names=target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n.b., can't seem to access files outside of Jupyter root, so have to create image in ./images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dot -Tpng /tmp/titanic_tree.dot -o images/titanic_tree.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = \"dot -Tpng {dotf} -o {pngf}\".format(dotf=out_file + \".dot\", pngf=\"images/\" + proj + \"_tree.png\")\n",
    "cmd\n",
    "\n",
    "import subprocess\n",
    "retval = subprocess.call(cmd, shell=True)\n",
    "retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"/tmp/titanic_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![titantic](images/titanic_tree.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decision_trees_helper as dthelp\n",
    "%aimport decision_trees_helper\n",
    "\n",
    "th_logit = dthelp.TitanicHelper()\n",
    "\n",
    "target_name = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf = th_logit.make_logit_clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SexToInt:transform: Cheating alert!, X has 2 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.77653631, 0.81005587, 0.7752809 , 0.75280899, 0.81355932])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_logit.fit(logit_clf, train_data, target_name)\n",
    "th_logit.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SexToInt:transform: Cheating alert!, X has 2 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.80446927, 0.80446927, 0.78651685, 0.75280899, 0.78531073])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_dt = dthelp.TitanicHelper()\n",
    "\n",
    "tree_clf = th.make_tree_clf()\n",
    "th_dt.fit(tree_clf, train_data, target_name)\n",
    "th_dt.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dot_file': 'dt.dot', 'png_file': 'dt.png', 'dot cmd rc': 0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_dt.export_tree(tree_clf, \"dt\", th_dt.feature_names, [ \"No\", \"Yes\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt.png](dt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    468\n",
       "1    109\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1    16\n",
       "0     8\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_dt, y_train_dt) = (th_dt.X_train, th_dt.y_train)\n",
    "\n",
    "y_train_dt.value_counts()\n",
    "y_train_dt[ X_train_dt[ \"Sex\" ] < 0.5 ].value_counts()\n",
    "y_train_dt[ (X_train_dt[ \"Sex\"] < 0.5) & ( X_train_dt[ \"Age\" ] <= 6.5)    ].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering column Sex on 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    468\n",
       "1    109\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering column Sex on 0.5\n",
      "Filtering column Age on 6.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    16\n",
       "0     8\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering column Age on 6.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    16\n",
       "0     8\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    460\n",
       "1     93\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_f, y_f, X_fn, y_fn   = th_dt.partition(X_train_dt, y_train_dt, [ (\"Sex\", 0.5 ) ] )\n",
    "y_f.value_counts()\n",
    "\n",
    "X_f2, y_f2, X_f2n, y_f2n = th_dt.partition(X_train_dt, y_train_dt, [ (\"Sex\", 0.5 ), (\"Age\", 6.5) ] )\n",
    "y_f2.value_counts()\n",
    "\n",
    "# Run the 2 level i 1 step increments\n",
    "X_f21, y_f21, X_f21n, y_f21n = th_dt.partition( X_f, y_f, [ (\"Age\", 6.5)])\n",
    "y_f21.value_counts()\n",
    "y_f21n.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    16\n",
       "0     8\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_f2.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
