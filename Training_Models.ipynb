{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip</b> \n",
    "\n",
    "My lecture notebooks to date have been cluttered with inline code.\n",
    "This can obscure the message.\n",
    "\n",
    "From now on: putting all my code in a module so that the notebook is more focussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import training_models_helper as tmh\n",
    "%aimport training_models_helper\n",
    "\n",
    "tm = tmh.TrainingModelsHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Better understand Transformations\n",
    "- Understand what the coefficients in Linear Regression mean\n",
    "- Some pitfalls of Categoriacal Features in Linear Regression\n",
    "- Optimization of the objective function\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "\n",
    "- We revisit Synthesizing Features, a type of Feature Engineering\n",
    "- In the previous lecture we introduced Transformations\n",
    "    - We motivated Transformations via the need for a Scaling transformation when using KNN\n",
    "    - We continue motivating other transformations (mainly in the context of Linear models)\n",
    "        - range reduction: influential points\n",
    "        - normality inducing transformations\n",
    "- Transformations may be applied either to the Features or Targets or both\n",
    "- We examine what the coefficients in Linear models are telling us\n",
    "    - Particular attention t Categorial features\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression: Matrix form refresher\n",
    "\n",
    "We have been writing our linear models as either\n",
    "\n",
    "$\n",
    "\\hat{y} = \\theta_0 + \\sum_{i=1}^n { \\theta_i \\cdot x_i }\n",
    "$\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$\n",
    "y = \\Theta^T \\cdot X\n",
    "$\n",
    "\n",
    "for \n",
    "- column vector $y$\n",
    "- column vector of coefficients $\\Theta$\n",
    "- feature matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's be very clear about dimensions\n",
    "- $y$ is of length $m$, the number of observations\n",
    "- $\\Theta$ is of length $n + 1$, since $\\Theta = [ \\theta_0, \\ldots, \\theta_n ]$\n",
    "- X is of dimension $(m, n+1)$\n",
    "    - one row per observation\n",
    "    - row $i$ of X is the feature vector for the $i^{th}$ observation\n",
    "    - element $i$ of y is the value of the target for the $i^{th}$ observation\n",
    "    - the first column of $X$ is a column of $1$'s, corresponding to the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations continued: synthesized features\n",
    "[Geron housing data](external/handson-ml/02_end_to_end_machine_learning_project.ipynb#Get-the-data)\n",
    " \n",
    "\n",
    "## Data: California Housing Prices Data (AG Chapt 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = tm.load_housing_data()\n",
    "housing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each row represents a district\n",
    "\n",
    "Goal is to predict Median House Value in the district.\n",
    "\n",
    "- Features\n",
    "    - total_rooms: number of rooms in the district\n",
    "    - total_bedrooms: number of bedrooms in the district\n",
    "    - households; number of households in the district\n",
    "    - population: number of people in the district\n",
    "    - median_income: median income of people in the district\n",
    "    - \n",
    "- Target\n",
    "    - median_house_value: median value of a house in the district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What potential issues pop out ?\n",
    "\n",
    "Some *raw* features depend on the *size* of the district, so can't compare the same feature across districts.\n",
    "\n",
    "- total_rooms, total_bedrooms, households, population \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering: transform to meaningful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By sythesizing ratios:\n",
    "- rooms/household\n",
    "- population/household\n",
    "- bedrooms/room\n",
    "\n",
    "we have transformed the raw features into processed features that will probably be better predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering: bucketing and clipping\n",
    "\n",
    "- Will $1 more of income *really* predict higher housing prices in the district ?\n",
    "    - create income buckets\n",
    "        - Disclaimer\n",
    "            - In the book this is **not** a feature but something used to \"stratify\" the sample\n",
    "            - We are taking a bit of artistic license to make a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median income ranges from 0.5 to 15.0\n",
      "Income buckets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0     7236\n",
       "2.0     6581\n",
       "4.0     3639\n",
       "5.0     1423\n",
       "1.0      822\n",
       "6.0      532\n",
       "7.0      189\n",
       "8.0      105\n",
       "9.0       50\n",
       "11.0      49\n",
       "10.0      14\n",
       "Name: income_cat, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Median income ranges from {min:.1f} to {max:.1f}\".format(min=housing[\"median_income\"].min(), \n",
    "                                                                max=housing[\"median_income\"].max()\n",
    "                                                               ))\n",
    "# Divide by 1.5 to limit the number of income categories\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "\n",
    "print(\"Income buckets\")\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Still a lot of buckets.  One theory is that incomes above bucket 5 don't predict housing prices, so clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income buckets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0    7236\n",
       "2.0    6581\n",
       "4.0    3639\n",
       "5.0    2362\n",
       "1.0     822\n",
       "Name: income_cat_clipped, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label those above 5 as 5\n",
    "housing[\"income_cat_clipped\"] = housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0)\n",
    "print(\"Income buckets\")\n",
    "housing[\"income_cat_clipped\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations continued: the importance of scaling\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "    <li>We continue to use Linear models to make our points.</li>\n",
    "    <li>It's important to emphasize once more that some of these points generalize to other models, and some don't</li>\n",
    "    <li>Linear models are sufficiently useful that we will invest the time even in the cases where the points don't fully generalize</li>\n",
    "    \n",
    " </ul>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stretched datasets: KNN affected, Decision Trees not\n",
    "\n",
    "Recall our previous example illustrating how some models are affected/unaffected by scaling transformations:\n",
    "- \"Stretch\" Feature 1 (multiply by 10)\n",
    "- Nearest neighbors is affected\n",
    "- Decision Tree is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kn = tmh.KNN_Helper()\n",
    "\n",
    "_ = kn.plot_classifiers(scale=False, num_ds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features with extreme values: Influential points\n",
    "\n",
    "We enumerated a number of transformations whose objective was to reduce the range of values for a feature.\n",
    "Reasons\n",
    "- put multiple features on same scale\n",
    "- reduce the influence of extreme observations\n",
    "\n",
    "Transformations\n",
    "- Standardize\n",
    "- MinMax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Influential points\n",
    "\n",
    "Some models may be quite sensitive to just a few observations, including Regression.\n",
    "\n",
    "Loosely speaking, an observation is **influential** if the parameter estimate $\\Theta$ changes greatly depending on whether the observation is included/excluded\n",
    "\n",
    "The **leverage** of an observation is related to the value of a feature in relation to the mean (across observations) of the feature\n",
    "- extreme values of the feature have higher leverage\n",
    "\n",
    "It is not always the case, but high leverage sometimes makes the point influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Influence of a point is a function of its leverage and how far its target is from the mean across targets of other observations.\n",
    "\n",
    "We will gain some intuition (hopefully) through the following interactive tool, which changes a single observation and refits a Linear model.\n",
    "- x: slider to control the index of the point being changed (left-most point is index 1; right-most is 10)\n",
    "- y: slider to control how much the target of the observation chosen is to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iph = tmh.InfluentialHelper()\n",
    "\n",
    "x,y = iph.setup()\n",
    "iph.show_slider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normality inducing transformations\n",
    "\n",
    "Recall that, for `LogisticRegression` we transformed the target from a probability (binary target, a 0/1 value)\n",
    "to log odds.\n",
    "\n",
    "- Note: target Survived/Not Survived is treated as a probability and has value of either 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the following Linear models to predict probability of Surviving Titanic\n",
    "- $ p^{(i)} = \\Theta^T \\cdot x^{(i)}$\n",
    "    - $p$ is not normally distributed, so hard for errors of linear model to be normal\n",
    "- $ { { p^{(i)} } \\over { 1 -p^{(i)}} } = \\Theta^T \\cdot x^{(i)}$\n",
    "    - odds are not normally distributed either\n",
    "-  $ { \\text{log}  {{ p^{(i)}  } \\over { 1 -p^{(i)}} } } = \\Theta^T \\cdot x^{(i)}$\n",
    "    - log odds *is* normally distributed as we show below\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So by synthesizing the target (the odds) and performing a Log transformation, we have a target that satisfies the\n",
    "assumptions of a Linear model.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "    <li> Loose Language Alert\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "- $ { { p^{(i)} } \\over { 1 -p^{(i)}} } $ is called **odds**\n",
    "- if I have two odds $O_i$ and $O_j$, then $ O_i \\over O_j$ is called an **odds ratio**\n",
    "\n",
    "I may have been sloppy in calling \"odds\" the \"odds ratio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf = tmh.TransformHelper()\n",
    "tf.plot_odds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretting the coefficients in Linear Models\n",
    "\n",
    "This section applies only to Linear models (`LinearRegression`, `LogisticRegression`)\n",
    "\n",
    "It may seem overly specialized, but since these models are used so often, we will spend some time.\n",
    "\n",
    "Also, because we can assign a meaning to the coefficients, these models are highly interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Numeric features\n",
    "For Linear Regression\n",
    "\n",
    "$\n",
    "\\hat{y^{(i)}} =  \\Theta \\cdot x^{(i)} = \\sum_{j=1} { \\theta_j * x^{(i)}_j }\n",
    "$\n",
    "\n",
    "so for a *unit* change in $x^{(i)}_j, \\Delta x^{(i)}_j $\n",
    "\n",
    "$\n",
    "\\Delta \\hat{y^{(i)}}  =  \\theta_j \\times \\Delta x^{(i)}_j \n",
    "$\n",
    "\n",
    "$\n",
    "\\theta_j = { \\partial{\\hat{y^{(i)}}} \\over { \\partial{ x^{(i)}_j} } }\n",
    "$\n",
    "\n",
    "That is, the coefficient $j$ is the amount $\\hat{y^{(i)}}$ changes for a $1$ unit change in $x^{(i)}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Transformations create **new** features/target and $\\theta$ expresses the change in prediction per unit change in feature.  \n",
    "\n",
    "Any transformed feature/target is expressed in **transformed** units, not original units.\n",
    "\n",
    "At test time:\n",
    "- transform test features\n",
    "- if the target has been transformed\n",
    "    - prediction units are in transformed units\n",
    "    - use an inverse transformation on the target to get back to original units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples\n",
    "- Log transform of target: \n",
    "    - $\\text{log}(y) =   \\theta_0 + \\theta_1 * x_1  $\n",
    "    - $\\theta_1 = { \\partial{ \\text{log}(y) } \\over { \\partial{ x_1 } } } = \\% $ change in $y$ per unit change in $x_1$\n",
    "- Log transform of both target and feature:\n",
    "    - $ \\text{log}(y) =  \\theta_0 + \\theta_1 * \\text{log}(x_1) $   \n",
    "    - $\\theta_1 = { \\partial{ \\text{log}(y) } \\over { \\partial{ \\text{log}(x_1) } } } = \\% $ change in $y$ per $\\%$ change in $x_1$\n",
    "    \n",
    "- Standardize feature\n",
    "    - Transform $x^{(i)}$ into $z_x^{(i)} = { { x^{(i)} - \\bar{x} } \\over { \\sigma_x } }$\n",
    "    - $y = \\theta_0 + \\theta_1 * z_x$\n",
    "    - $\\theta_1 = $ change in $y$ per $1$ standard deviation change in $x$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remember**\n",
    "- if you transform features in training, you must apply the same transformation to features in test\n",
    "    - if the transformation is parameterized, the parameters are determined at **train** fit time, not test !\n",
    "- if you transform the target, the prediction is in different units than the original\n",
    "    - you can perform the inverse transformation to get a prediction in original units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical features\n",
    "\n",
    "Consider the simplified example of numerical features $X$ and a single binary categorical feature $c$ and linear model\n",
    "\n",
    "$\n",
    "y = \\Theta^T X + \\theta_c c\n",
    "$\n",
    "\n",
    "Let's assume (just for the moment) that we represent $c$ as a binary variable, rather than use one-hot encoding:\n",
    "\n",
    "$c^{(i)} \\in \\{ 0, 1 \\}$ for all $i$\n",
    "\n",
    "So $\\theta_c$ is the increase in $y$ when $c^{(i)} = 1$ compared to when $c^{(i)} = 1$\n",
    "\n",
    "Just like with numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's wrong with representing multinomial categorical values as numbers ?\n",
    "\n",
    "Let's consider the Passenger Class (PClass) variable from the Titanic example:\n",
    "\n",
    "$\n",
    "\\text{Pclass} \\in \\{ 1, 2, 3 \\}\n",
    "$\n",
    "\n",
    "Now that you know the interpretation of $\\theta_{\\text{Pclass}}$\n",
    "- the difference in prediction for $(Pclass = 1)$ vs $(PClass = 2)$, or $(Pclass = 2)$ vs $(Pclass = 3)$ is $\\theta_{\\text{Pclass}}$\n",
    "- BUT the difference in prediction for $Pclass = 1$ vs $(PClass = 3)$ is $ 2 \\times \\theta_{\\text{Pclass}}$\n",
    "    - twice the impact: is this really true ?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - What if $\\text{Pclass} \\in \\{ 100, 200, 300 \\}$ ?\n",
    "- Numeric values imply both\n",
    "    - an ordering\n",
    "    - and a magnitude\n",
    "    \n",
    "Unless the order and magnitude match your semantics, use binary values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The \"Dummy variable trap\" for Linear Models\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    This is a trap only for Linear Models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With one-hot encoding, for each possible value of categorical feature $c$, we add a new feature.\n",
    "\n",
    "Suppose for multinomial $c$, the possible values for $c$ are such that $c \\in C$ where\n",
    "\n",
    "$C =  \\{ c_1, c_2, \\ldots , c_n \\}$\n",
    "\n",
    "Let\n",
    "\n",
    "$\n",
    "1_{c=c_i}\n",
    "$\n",
    "denote the feature \"$c$ equals $c_i$\"\n",
    "\n",
    "For example, if $c$ is a categorical feature for Sex ($c \\in \\{ Male, Female \\}$)\n",
    "one-hot encoding of Sex adds two features\n",
    "- $1_{c=\\text{Male}}$\n",
    "- $1_{c=\\text{Female}}$\n",
    "\n",
    "(n.b., we had been writing these on the board as \"$\\text{Is}_{\\text{Male}}$\" and \"$\\text{Is}_{\\text{Female}}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so our model becomes\n",
    "\n",
    "$\n",
    "y = \\Theta^T X + \\sum_{v \\in C}{ (\\theta_{c=v} \\cdot 1_{c = v}) }\n",
    "$\n",
    "\n",
    "Let us call the sub-model without the categorical features the \"reference (or base) model\", i.e.\n",
    "\n",
    "$\n",
    "y = \\Theta^T X \n",
    "$\n",
    "\n",
    "In the case of $𝑐∈{𝑀𝑎𝑙𝑒,𝐹𝑒𝑚𝑎𝑙𝑒}$\n",
    "\n",
    "- $\\theta_{c=\\text{Male}}$ is the increase in $y$ when $c^{(i)} = \\text{Male}$ compared to the reference model\n",
    "- $\\theta_{c=\\text{Female}}$ is the increase in $y$ when $c^{(i)} = \\text{Female}$ compared to the reference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "See the problem ?\n",
    "The reference model\n",
    "\n",
    "$y = \\Theta^T X$ \n",
    "\n",
    "corresponds to the value of $y$ when $c^{(i)}$ is neither Male nor Female !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It gets even worse.\n",
    "\n",
    "For every observation $i$, \n",
    "\n",
    "$\\sum_{v \\in C} {1_{c=v}} = 1$\n",
    "\n",
    "That means the set of features created by the one-hot encoding $ \\{ {1_{c=v} | v \\in C}\\}$ are\n",
    "**co-linear** with the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have fallen into what is known in Linear models as *The Dummy Variable Trap*\n",
    "\n",
    "The way out of the trap is simple:\n",
    "- Omit the feature $1_{c=c_j}$ for *one* value $c_j \\in C$\n",
    "- The reference model $y = \\Theta^T X$ is now intepreted as the value of $y$ \n",
    "    - when all of the non-omited features are False\n",
    "    - hence, when the omited feature is True\n",
    "- $\\theta_{c = c_k}, k \\ne j$ is the increment over the *reference model* when $c^{(i)} = c_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lesson**\n",
    "For categorical features in Linear models\n",
    "- create binary indicator features for *all but one* value in the category\n",
    "\n",
    "Why didn't we encounter this problem before ?\n",
    "- We never used a Linear model with categorical features\n",
    "- We *did* use a Logistic Regression model with categorical features\n",
    "    - sklearn's `LogisticRegression` defaults to penalized regression\n",
    "        - the penalty is mitigating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NOTE**\n",
    "The Dummy Variable Trap is *only* a problem for Linear models.\n",
    "\n",
    "One-hot encoding works fine for just about every other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting the MNIST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mnist_helper as mnh\n",
    "%aimport mnist_helper\n",
    "\n",
    "mn = mnh.MNIST_Helper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the training dtaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mn.setup()\n",
    "mn.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit a `LogisticRegression` model and examine the coefficients $\\Theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mnist_lr = mn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mnist_fig, mnist_ax = mn.plot_coeff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a model\n",
    "\n",
    "Our model (hypothesis) is written as\n",
    "\n",
    "$\n",
    "\\hat{y^{(i)}} = h_\\Theta(x^{(i)})\n",
    "$\n",
    "\n",
    "That is, our prediction for feature vector $x^{(i)}$ is a function with parameters $\\Theta$.\n",
    "\n",
    "**Model fitting** takes the training data and solves for $\\Theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fitting most models usually involves the solution of an **Optimization Objective**.\n",
    "- If we are minimizing: the optimization objective is called the **cost** function\n",
    "- If we are maximizing: the optimization objective is called the **utility** function\n",
    "\n",
    "The basic purpose of the optimization objective is to cause predictions to be close to the true values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us denote\n",
    "\n",
    "$\n",
    "\\text{error}^{(i)} = \\hat{y^{(i)}} - y^{(i)}\n",
    "$\n",
    "\n",
    "So an Optimizaton Objective (Cost Function) that minimizes errors is one that seeks to make predictions close to true values.\n",
    "\n",
    "There may be added elements (e.g., constraints) of the objective as well (discussed later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradients\n",
    "\n",
    "Gradient Descent is a method for optimizing the Optimization Objective.\n",
    "\n",
    "It works for any model but we will illustrate it with Linear Regression.\n",
    "\n",
    "For Linear Regression, the Cost Function is\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\text{MSE}(X, \\Theta) = {1 \\over m} \\sum_{i=1}^m { (\\text{error}^{(i)} )^2}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that one way to minimize a Cost Function is to take derivatives with respect to $\\Theta$ and set them to $0$.\n",
    "\n",
    "Because $\\Theta$ is a vector, there is one derivative per feature.\n",
    "Hence the vector of derivatives (called the **gradient**) is\n",
    "\n",
    "$\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\, \\text{MSE}(X, \\boldsymbol{\\theta}) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(X, \\boldsymbol{\\theta}) \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_1} \\text{MSE}(X, \\boldsymbol{\\theta}) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(X, \\boldsymbol{\\theta})\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{array}{lll}\n",
    "{ { \\partial  }\\over { \\partial{ \\theta_j} } } { \\text{MSE}(X, \\Theta) }& = & {1 \\over m} \\sum_{i=1}^m { \\partial \\over { \\partial \\theta_j } } { ( \\text{error}^{(i)})^2 }\\\\\n",
    "& = & {1 \\over m} \\sum_{i=1}^m { 2 \\times \\text{error}^{(i)} \\times { \\partial \\over { \\theta_j} }} {\\text{error}^{(i)} }\\\\\n",
    "& = & {2 \\over m} \\sum_{i=1}^m { \\text{error}^{(i)} \\times { \\partial \\over { \\theta_j} }} { \\hat{y^{(i)} } } \\\\\n",
    "&= &  {2 \\over m} \\sum_{i=1}^m { \\text{error}^{(i)} \\times { x^{(i)}_j} }\\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Linear Regression\n",
    "\n",
    "$\n",
    "\\hat{y^{(i)}} =  \\Theta^T \\cdot x^{(i)} = \\sum_{j=0}^n { \\theta_j * x^{(i)}_j }\n",
    "$\n",
    "\n",
    "so\n",
    "\n",
    "$\n",
    "\\text{error}^{(i)} = \\Theta^T \\cdot x^{(i)} - y^{(i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the gradient for Linear Regression can be written in matrix form as\n",
    "\n",
    "$\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\, \\text{MSE}(X, \\boldsymbol{\\theta}) =\n",
    " = \\dfrac{2}{m} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y})\n",
    "$\n",
    "\n",
    "This will be particularly useful when working with NumPy as the gradient calculation is a vector operation that is implemented so as to be fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "The basic algorithm is:\n",
    "1. Initialize $\\Theta$ randomly\n",
    "1. Repeat until done\n",
    "    1. Compute the Gradient\n",
    "    1. Update $\\Theta$ by taking a step in the (negative) direction of the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's illustrate Batch Gradient Descent on an example.\n",
    "\n",
    "First, we use sklearn's `LinearRegression` as a baseline against which we will compare the $\\Theta$ obtained from\n",
    "Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd = tmh.GradientDescentHelper()\n",
    "\n",
    "X_lr, y_lr = gd.gen_lr_data()\n",
    "clf_lr = gd.fit_lr(X_lr,y_lr)\n",
    "fig, ax = gd.plot_lr(X_lr, y_lr, clf_lr)\n",
    "\n",
    "theta_lr = (clf_lr.intercept_, clf_lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's perform Batch Gradient Descent and compare the $\\Theta$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd_theta = gd.batchGradientDescent_lr(X_lr, y_lr)\n",
    "theta_lr - gd_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $\\Theta$'s are equal up to 15 decimal points.\n",
    "\n",
    "Let's look at the code for Batch Gradient Descent and examine the details"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#gd.batchGradientDescent_lr??\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can see the code that implements the steps described in English\n",
    "    - `eta` is the step size: how fast we adjust $\\Theta$ in the direction of the gradient\n",
    "    - `X_b` is the matrix\n",
    "        - whose first column is $1$\n",
    "        - whose other columns are the non-intercept features\n",
    "    - `X_b.dot(theta)` are the predicted values for all observations\n",
    "    - `X_b.dot(theta) - y` are the errors for all observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since the $\\Theta$'s are the same, it's no surprise that the predictions are too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "gd_y_pred = gd.predict(X_new, theta_lr)\n",
    "clf_y_pred = clf_lr.predict(X_new)\n",
    "\n",
    "gd_y_pred == clf_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch gradient descent: the movie\n",
    "\n",
    "Let's watch Batch Gradient Descent at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "gd_anim = gd.create_movie(X_lr, y_lr, n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd.show_movie(gd_anim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initializing $\\Theta$\n",
    "What would have happened if, instead of initializing $\\Theta$ to random numbers we had initialized it to $0$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step size\n",
    "\n",
    "What's a good choice for `eta` ?  We had used 0.1 and obtained convergence in around 10 steps.\n",
    "\n",
    "Le'ts try a smaller step size: `eta` = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "gd_anim_eta_02 = gd.create_movie(X_lr, y_lr, eta=0.02, n_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd.show_movie(gd_anim_eta_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like watching paint dry !\n",
    "\n",
    "How about something bigger ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "gd_anim_eta_45 = gd.create_movie(X_lr, y_lr, eta=0.45, n_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd.show_movie(gd_anim_eta_45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And even bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "gd_anim_eta_50 = gd.create_movie(X_lr, y_lr, eta=0.50, n_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd.show_movie(gd_anim_eta_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lost in space !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning rate schedule\n",
    "\n",
    "We see from the above that if the step size is too small, it takes long to converge.\n",
    "\n",
    "But if the step size is too big, we may overshoot.\n",
    "\n",
    "An adaptive learning rate schedule may be the solution:\n",
    "- take big steps at first\n",
    "- take smaller steps toward end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = np.linspace(0, 10, 10)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(1,1,1)\n",
    "_ =ax.plot(t, learning_schedule(t))\n",
    "_ = ax.set_xlabel(\"time\")\n",
    "_ = ax.set_ylabel(\"eta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to stop\n",
    "\n",
    "Can we do better than running for a fixed number of iterations ?\n",
    "Yes:\n",
    "- Let $\\text{Cost}_{t}$ be the Cpst Function at step $t$\n",
    "- Stop if\n",
    "    - $\\text{Cost}_{t-1} - \\text{Cost}_{t} < \\epsilon $\n",
    "    - That is: stop if improvement of Cost Function is not big enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "What is the computational complexity of Batch Gradient Descent (for Linear Regresssion_?\n",
    "\n",
    "Can you spot the bottle-neck ?\n",
    "\n",
    "$$ \\nabla(j) = w^{(j)} + C \\sum_{i=1}^n { \\frac{\\partial L(x_i, y_i)} {\\partial w^{(j)}} }$$\n",
    "\n",
    "\n",
    "Evaluating MSE (and the derivatives) involves iterating over all $m$ observations in the train dataset.\n",
    "\n",
    "This can be quite large and hence slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stochastic Gradient Descent evaluates the gradient at a single, randomly chosen point\n",
    "\n",
    "$$ \\nabla(j,i) = w^{(j)} + C { \\frac{\\partial L(x_i, y_i)} {\\partial w^{(j)}} }$$\n",
    "\n",
    "- so takes lots of steps\n",
    "- each pass through $m$ observations is called an **epoch**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        # Choose one observation at random\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        \n",
    "        # Evaluate gradient at the observation\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        \n",
    "        # Update theta\n",
    "        theta = theta - eta * gradients\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minbatch Gradient Descent\n",
    "\n",
    "Our original  Batch Gradient Descent examined all $m$ observatons in the training set in order to\n",
    "compute the exact value of the derivatives.\n",
    "\n",
    "Stocahstic Gradient Descent evaluated the derivative at a single point.\n",
    "- this can be quite noisy\n",
    "\n",
    "We can get a pretty good, less-noisy estimate of the derivatives by examining a **batch** of observations whose size is more than 1 but fewer than $m$.\n",
    "\n",
    "This is called Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "    \n",
    "t = 0    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "for epoch in range(n_iterations):\n",
    "    # Shuffle the observations for each epoch\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    \n",
    "    # Evaluate/update in batches of size minibatch_size\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        \n",
    "        # Grab a batch of observations at indices [i:i+minibatch_size]\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        \n",
    "        # Evalute the gradient over the batch\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        \n",
    "        # Update theta\n",
    "        theta = theta - eta * gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe that Batch Gradient Descent (our original attempt) is Minibatch Gradient Descent with `minibatch_size = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other cost functions\n",
    "\n",
    "- Ridge Regression Cost Function\n",
    "    - MSE, with a penalty large $\\Theta$\n",
    "        - it's easy to compute the derivative of this cost function\n",
    "        - try Minibatch Gradient Descent on this Cost Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A word on derivatives\n",
    "\n",
    "Preview of part 2 of the course:\n",
    "- the derivatives we used were *analytic* and not numerical approximations\n",
    "- how can we automate calculation of analytic derivatives ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cool cost functions\n",
    "- Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross entropy, KL divergence\n",
    "\n",
    "A measure of distribution similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost function for Logistic Regression\n",
    "\n",
    "Consider a single observation with target $y$\n",
    "\n",
    "We assign the following cost to our prediction $\\hat{y}$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "c(\\theta) & = &\n",
    "\\left\\{\n",
    "{\n",
    "    \\begin{array}{ll}\n",
    "    - \\textrm{log}(\\hat{p}) & \\textrm{if } & y = 1 \\\\\n",
    "    - \\textrm{log}(1-\\hat{p})     & \\textrm{if } & y = 0 \\\\\n",
    "    \\end{array}\n",
    "}\n",
    "\\right.\n",
    "& = & - \\left( y*\\textrm{log}(\\hat{p}) + (1-y) * \\textrm{log}(1-\\hat{p}) \\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "and over the entire training set of size $m$\n",
    "\n",
    "$$\n",
    "J(\\theta) = \n",
    "\\frac{1}{m} \n",
    "\\sum_{i=1}^m \n",
    "\\left(\n",
    "{ y^i * \\textrm{log} (\\hat{p}^i) + (1-y^i) * \\textrm{log}(1-\\hat{p}^i)}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Intuition**\n",
    "\n",
    "- if $y^i = 1$\n",
    "    - the second addend is 0\n",
    "    - we want the first addend to be small. i.e.,\n",
    "        - $\\hat{p}^i$ to be $1$, so that $\\textrm{log}( \\hat{p}^i) = 0$\n",
    "- if $y^i = 0$\n",
    "    - the first addend is 0\n",
    "        - we want the second addend to be small, i.e., \n",
    "            - $\\hat{p}^i$ to be $0$, so that  $\\textrm{log}( 1 - \\hat{p}^i) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
